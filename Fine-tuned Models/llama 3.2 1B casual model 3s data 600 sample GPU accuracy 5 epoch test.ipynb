{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ad6af4-61b4-4d64-aaaf-02d0ad20c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780fa92c-6b1f-4c0c-929a-6f0fb4b01509",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cace7a-1048-466c-b027-6485c41c76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\cheng\\Desktop\\202A Project\\UCI HAR Dataset\\UCI HAR Dataset\\train\\y_train.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        y_train.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39cf573-a7e3-4373-b869-88aa29d88deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r'C:\\Users\\cheng\\Desktop\\202A Project\\UCI HAR Dataset\\UCI HAR Dataset\\train\\Inertial Signals'\n",
    "\n",
    "train_data = {}\n",
    "train_names = []\n",
    "interval = 5\n",
    "num_sample = 3\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        file_lines = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = list(map(lambda x: round(float(x), 5), line.strip().split()))\n",
    "                file_lines.append(list(line[interval * x] for x in range(num_sample)))\n",
    "        train_data[filename[:-4]] = file_lines\n",
    "        train_names.append(filename[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ccdac2-e2fc-4fea-bdd6-b1783c3b284b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_train</th>\n",
       "      <th>body_acc_y_train</th>\n",
       "      <th>body_acc_z_train</th>\n",
       "      <th>body_gyro_x_train</th>\n",
       "      <th>body_gyro_y_train</th>\n",
       "      <th>body_gyro_z_train</th>\n",
       "      <th>total_acc_x_train</th>\n",
       "      <th>total_acc_y_train</th>\n",
       "      <th>total_acc_z_train</th>\n",
       "      <th>y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.00018, 0.00405, 0.00541]</td>\n",
       "      <td>[0.01077, 0.00694, 0.00589]</td>\n",
       "      <td>[0.05556, 0.04473, 0.021]</td>\n",
       "      <td>[0.03019, 0.05018, 0.04649]</td>\n",
       "      <td>[0.06601, 0.06917, 0.05578]</td>\n",
       "      <td>[0.02286, 0.00772, 0.0151]</td>\n",
       "      <td>[1.01282, 1.01697, 1.01864]</td>\n",
       "      <td>[-0.12322, -0.12446, -0.12314]</td>\n",
       "      <td>[0.10293, 0.10749, 0.09765]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.00109, 0.00242, -0.00049]</td>\n",
       "      <td>[-0.00469, -0.00543, -0.00151]</td>\n",
       "      <td>[-0.02686, -0.02632, -0.01899]</td>\n",
       "      <td>[0.01711, 0.02825, 0.00993]</td>\n",
       "      <td>[0.00612, 0.01593, 0.00787]</td>\n",
       "      <td>[0.01227, 0.00569, 0.01194]</td>\n",
       "      <td>[1.01885, 1.02052, 1.01793]</td>\n",
       "      <td>[-0.12398, -0.12492, -0.12132]</td>\n",
       "      <td>[0.09793, 0.09658, 0.10162]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.00353, -0.00315, -0.00648]</td>\n",
       "      <td>[0.00446, 0.00351, -0.00268]</td>\n",
       "      <td>[-0.00591, -0.00774, -0.00769]</td>\n",
       "      <td>[0.02619, 0.01463, 0.04937]</td>\n",
       "      <td>[-0.00024, -0.00305, -0.00708]</td>\n",
       "      <td>[0.00216, 0.00127, -0.0009]</td>\n",
       "      <td>[1.02313, 1.01645, 1.01312]</td>\n",
       "      <td>[-0.12002, -0.1213, -0.12778]</td>\n",
       "      <td>[0.09112, 0.08829, 0.08755]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.00177, 0.001, -0.00448]</td>\n",
       "      <td>[-0.01019, -0.00103, -0.00498]</td>\n",
       "      <td>[0.00105, -0.00561, 0.0007]</td>\n",
       "      <td>[-0.03752, -0.01018, 0.00328]</td>\n",
       "      <td>[-0.01289, -0.01244, -0.01975]</td>\n",
       "      <td>[-0.00087, 0.00902, 0.01842]</td>\n",
       "      <td>[1.01768, 1.02048, 1.01502]</td>\n",
       "      <td>[-0.1334, -0.12375, -0.12728]</td>\n",
       "      <td>[0.09515, 0.08893, 0.09565]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[9e-05, 0.00089, 0.00175]</td>\n",
       "      <td>[-0.00386, -0.00844, -0.00833]</td>\n",
       "      <td>[-0.01333, -0.00433, -0.00872]</td>\n",
       "      <td>[-0.01943, -0.02128, -0.01934]</td>\n",
       "      <td>[-0.00861, -0.00413, -0.00378]</td>\n",
       "      <td>[-0.00157, -0.00204, -0.00141]</td>\n",
       "      <td>[1.01995, 1.02087, 1.02185]</td>\n",
       "      <td>[-0.12873, -0.13399, -0.13457]</td>\n",
       "      <td>[0.08084, 0.0893, 0.08428]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                body_acc_x_train                body_acc_y_train  \\\n",
       "0    [0.00018, 0.00405, 0.00541]     [0.01077, 0.00694, 0.00589]   \n",
       "1   [0.00109, 0.00242, -0.00049]  [-0.00469, -0.00543, -0.00151]   \n",
       "2  [0.00353, -0.00315, -0.00648]    [0.00446, 0.00351, -0.00268]   \n",
       "3    [-0.00177, 0.001, -0.00448]  [-0.01019, -0.00103, -0.00498]   \n",
       "4      [9e-05, 0.00089, 0.00175]  [-0.00386, -0.00844, -0.00833]   \n",
       "\n",
       "                 body_acc_z_train               body_gyro_x_train  \\\n",
       "0       [0.05556, 0.04473, 0.021]     [0.03019, 0.05018, 0.04649]   \n",
       "1  [-0.02686, -0.02632, -0.01899]     [0.01711, 0.02825, 0.00993]   \n",
       "2  [-0.00591, -0.00774, -0.00769]     [0.02619, 0.01463, 0.04937]   \n",
       "3     [0.00105, -0.00561, 0.0007]   [-0.03752, -0.01018, 0.00328]   \n",
       "4  [-0.01333, -0.00433, -0.00872]  [-0.01943, -0.02128, -0.01934]   \n",
       "\n",
       "                body_gyro_y_train               body_gyro_z_train  \\\n",
       "0     [0.06601, 0.06917, 0.05578]      [0.02286, 0.00772, 0.0151]   \n",
       "1     [0.00612, 0.01593, 0.00787]     [0.01227, 0.00569, 0.01194]   \n",
       "2  [-0.00024, -0.00305, -0.00708]     [0.00216, 0.00127, -0.0009]   \n",
       "3  [-0.01289, -0.01244, -0.01975]    [-0.00087, 0.00902, 0.01842]   \n",
       "4  [-0.00861, -0.00413, -0.00378]  [-0.00157, -0.00204, -0.00141]   \n",
       "\n",
       "             total_acc_x_train               total_acc_y_train  \\\n",
       "0  [1.01282, 1.01697, 1.01864]  [-0.12322, -0.12446, -0.12314]   \n",
       "1  [1.01885, 1.02052, 1.01793]  [-0.12398, -0.12492, -0.12132]   \n",
       "2  [1.02313, 1.01645, 1.01312]   [-0.12002, -0.1213, -0.12778]   \n",
       "3  [1.01768, 1.02048, 1.01502]   [-0.1334, -0.12375, -0.12728]   \n",
       "4  [1.01995, 1.02087, 1.02185]  [-0.12873, -0.13399, -0.13457]   \n",
       "\n",
       "             total_acc_z_train  y_train  \n",
       "0  [0.10293, 0.10749, 0.09765]        5  \n",
       "1  [0.09793, 0.09658, 0.10162]        5  \n",
       "2  [0.09112, 0.08829, 0.08755]        5  \n",
       "3  [0.09515, 0.08893, 0.09565]        5  \n",
       "4   [0.08084, 0.0893, 0.08428]        5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in train_data.items()]))\n",
    "df['y_train'] = y_train\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a607d70-ba8d-4757-a5a7-b1bd43f56866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          text_input\n",
      "0  The data provided is measured and collected in...\n",
      "1  The data provided is measured and collected in...\n",
      "2  The data provided is measured and collected in...\n",
      "3  The data provided is measured and collected in...\n",
      "4  The data provided is measured and collected in...\n"
     ]
    }
   ],
   "source": [
    "activity_map = {\n",
    "    1: \"Walking\",\n",
    "    2: \"Walking Upstairs\",\n",
    "    3: \"Walking Downstairs\",\n",
    "    4: \"Sitting\",\n",
    "    5: \"Standing\",\n",
    "    6: \"Laying\"\n",
    "}\n",
    "def create_text_format(row):\n",
    "    imu_data = (\n",
    "        f\"The data provided is measured and collected in 2.56s.\\n\"\n",
    "        f\"body_acc_x: {row['body_acc_x_train']}, body_acc_y: {row['body_acc_y_train']}, body_acc_z: {row['body_acc_z_train']}, \"\n",
    "        f\"body_gyro_x: {row['body_gyro_x_train']}, body_gyro_y: {row['body_gyro_y_train']}, body_gyro_z: {row['body_gyro_z_train']}, \"\n",
    "        f\"Based on these numbers, classify what the person is doing during this 2.56s?\\n\"\n",
    "    )\n",
    "    return f\"{imu_data}\"\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['text_input'] = df.apply(create_text_format, axis=1)\n",
    "\n",
    "# Output a sample\n",
    "print(df[['text_input']].head())\n",
    "\n",
    "# Now the `df['text_input']` column contains the text-based data that you can feed into the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12418314-8859-4222-9b1a-c929bdc0f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardata = df.drop(['body_acc_x_train','body_acc_y_train','body_acc_z_train','body_gyro_x_train','body_gyro_y_train','body_gyro_z_train','total_acc_x_train','total_acc_y_train','total_acc_z_train'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "debe143e-3b32-4922-94ed-37cce355d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardata = hardata.rename(columns={'y_train': 'label'})\n",
    "hardata = hardata.rename(columns={'text_input': 'text'})\n",
    "hardata['label'] = hardata['label'] - 1\n",
    "hardata = hardata[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3753b82f-78ec-42a4-9e38-6f1e02529452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "0     The data provided is measured and collected in...      4\n",
      "1     The data provided is measured and collected in...      4\n",
      "2     The data provided is measured and collected in...      4\n",
      "3     The data provided is measured and collected in...      4\n",
      "4     The data provided is measured and collected in...      4\n",
      "...                                                 ...    ...\n",
      "7347  The data provided is measured and collected in...      1\n",
      "7348  The data provided is measured and collected in...      1\n",
      "7349  The data provided is measured and collected in...      1\n",
      "7350  The data provided is measured and collected in...      1\n",
      "7351  The data provided is measured and collected in...      1\n",
      "\n",
      "[7352 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hardata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc5c841-ce2d-4760-b5d8-93346669bb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheng\\AppData\\Local\\Temp\\ipykernel_9132\\2406313597.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  hardata = hardata.groupby('label').apply(lambda x: x.sample(100, replace=True) if len(x) > 150 else x)\n"
     ]
    }
   ],
   "source": [
    "hardata = hardata.groupby('label').apply(lambda x: x.sample(100, replace=True) if len(x) > 150 else x)\n",
    "label_counts = hardata['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63ef8997-bcd4-4263-82d9-fd8214299cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         text  label\n",
      "label                                                               \n",
      "0     3727  The data provided is measured and collected in...      0\n",
      "      7090  The data provided is measured and collected in...      0\n",
      "      5005  The data provided is measured and collected in...      0\n",
      "      270   The data provided is measured and collected in...      0\n",
      "      1998  The data provided is measured and collected in...      0\n",
      "...                                                       ...    ...\n",
      "5     6541  The data provided is measured and collected in...      5\n",
      "      1538  The data provided is measured and collected in...      5\n",
      "      744   The data provided is measured and collected in...      5\n",
      "      5584  The data provided is measured and collected in...      5\n",
      "      604   The data provided is measured and collected in...      5\n",
      "\n",
      "[600 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hardata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0496725-2c32-486a-9bea-f09b61820dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token here\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f65306a-438d-47a5-a472-16525ff78196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/5, Loss: 2.466633605957031\n",
      "Epoch 2/5, Loss: 1.9060798048973084\n",
      "Epoch 3/5, Loss: 1.9302313407262166\n",
      "Epoch 4/5, Loss: 1.7194589217503866\n",
      "Epoch 5/5, Loss: 1.4625765760739644\n",
      "Validation Accuracy: 0.2917\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class LlamaForSequenceClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.classifier = nn.Linear(base_model.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get hidden states from the causal model\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, return_dict=True)\n",
    "        \n",
    "        # Extract the last hidden layer (not logits)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Shape: [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "        # Use the hidden state of the last token in the sequence\n",
    "        last_hidden_state = hidden_states[:, -1, :]  # Shape: [batch_size, hidden_size]\n",
    "        \n",
    "        # Pass the last hidden state through the classification head\n",
    "        logits = self.classifier(last_hidden_state)  # Shape: [batch_size, num_labels]\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "\n",
    "\n",
    "# Tokenization and Dataset Class\n",
    "class HARDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "base_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Define the classification model\n",
    "num_labels = 6\n",
    "model = LlamaForSequenceClassification(base_model=base_model, num_labels=num_labels)\n",
    "\n",
    "# Prepare data (replace `hardata` with your actual dataset)\n",
    "texts = hardata['text'].tolist()\n",
    "labels = hardata['label'].tolist()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_data(texts, labels, tokenizer, max_length=512):\n",
    "    tokenized = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return tokenized, torch.tensor(labels)\n",
    "\n",
    "train_encodings, train_labels = tokenize_data(train_texts, train_labels, tokenizer)\n",
    "val_encodings, val_labels = tokenize_data(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HARDataset(train_encodings, train_labels)\n",
    "val_dataset = HARDataset(val_encodings, val_labels)\n",
    "\n",
    "# DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42432b6d-dff3-41e1-8ad2-b310f7c46824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu_5epoch\\\\tokenizer_config.json',\n",
       " './fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu_5epoch\\\\special_tokens_map.json',\n",
       " './fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu_5epoch\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.save_pretrained(\"./fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu_5epoch\")\n",
    "torch.save(model.classifier.state_dict(), \"./fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu_5epoch/classifier.pt\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu_5epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d907714-2cc6-46b0-b9e3-9116c76c52e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 1.2855201244354248\n",
      "Epoch 2/2, Loss: 1.4742148518562317\n",
      "Validation Accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78c462-f029-45b5-822e-d90e7e0ca085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
