{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ad6af4-61b4-4d64-aaaf-02d0ad20c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780fa92c-6b1f-4c0c-929a-6f0fb4b01509",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cace7a-1048-466c-b027-6485c41c76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\cheng\\Desktop\\202A Project\\UCI HAR Dataset\\UCI HAR Dataset\\train\\y_train.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        y_train.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39cf573-a7e3-4373-b869-88aa29d88deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r'C:\\Users\\cheng\\Desktop\\202A Project\\UCI HAR Dataset\\UCI HAR Dataset\\train\\Inertial Signals'\n",
    "\n",
    "train_data = {}\n",
    "train_names = []\n",
    "interval = 5\n",
    "num_sample = 3\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        file_lines = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = list(map(lambda x: \"{:.5e}\".format(float(x)), line.strip().split()))\n",
    "                file_lines.append(list(line[interval * x] for x in range(num_sample)))\n",
    "        train_data[filename[:-4]] = file_lines\n",
    "        train_names.append(filename[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ccdac2-e2fc-4fea-bdd6-b1783c3b284b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_train</th>\n",
       "      <th>body_acc_y_train</th>\n",
       "      <th>body_acc_z_train</th>\n",
       "      <th>body_gyro_x_train</th>\n",
       "      <th>body_gyro_y_train</th>\n",
       "      <th>body_gyro_z_train</th>\n",
       "      <th>total_acc_x_train</th>\n",
       "      <th>total_acc_y_train</th>\n",
       "      <th>total_acc_z_train</th>\n",
       "      <th>y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.80852e-04, 4.04510e-03, 5.40712e-03]</td>\n",
       "      <td>[1.07668e-02, 6.94432e-03, 5.89227e-03]</td>\n",
       "      <td>[5.55607e-02, 4.47293e-02, 2.10005e-02]</td>\n",
       "      <td>[3.01912e-02, 5.01847e-02, 4.64883e-02]</td>\n",
       "      <td>[6.60136e-02, 6.91739e-02, 5.57777e-02]</td>\n",
       "      <td>[2.28586e-02, 7.72471e-03, 1.50980e-02]</td>\n",
       "      <td>[1.01282e+00, 1.01697e+00, 1.01864e+00]</td>\n",
       "      <td>[-1.23217e-01, -1.24462e-01, -1.23138e-01]</td>\n",
       "      <td>[1.02934e-01, 1.07493e-01, 9.76467e-02]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.09375e-03, 2.41589e-03, -4.87023e-04]</td>\n",
       "      <td>[-4.68759e-03, -5.42598e-03, -1.51494e-03]</td>\n",
       "      <td>[-2.68595e-02, -2.63214e-02, -1.89863e-02]</td>\n",
       "      <td>[1.71111e-02, 2.82541e-02, 9.92908e-03]</td>\n",
       "      <td>[6.12280e-03, 1.59305e-02, 7.87224e-03]</td>\n",
       "      <td>[1.22682e-02, 5.68509e-03, 1.19422e-02]</td>\n",
       "      <td>[1.01885e+00, 1.02052e+00, 1.01793e+00]</td>\n",
       "      <td>[-1.23976e-01, -1.24919e-01, -1.21316e-01]</td>\n",
       "      <td>[9.79296e-02, 9.65768e-02, 1.01621e-01]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3.53127e-03, -3.14845e-03, -6.47614e-03]</td>\n",
       "      <td>[4.45594e-03, 3.51417e-03, -2.68002e-03]</td>\n",
       "      <td>[-5.91458e-03, -7.74461e-03, -7.68998e-03]</td>\n",
       "      <td>[2.61888e-02, 1.46342e-02, 4.93729e-02]</td>\n",
       "      <td>[-2.38341e-04, -3.04995e-03, -7.07990e-03]</td>\n",
       "      <td>[2.15890e-03, 1.26839e-03, -9.01401e-04]</td>\n",
       "      <td>[1.02313e+00, 1.01645e+00, 1.01312e+00]</td>\n",
       "      <td>[-1.20016e-01, -1.21304e-01, -1.27783e-01]</td>\n",
       "      <td>[9.11167e-02, 8.82905e-02, 8.75458e-02]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-1.77235e-03, 9.98156e-04, -4.48179e-03]</td>\n",
       "      <td>[-1.01854e-02, -1.03379e-03, -4.97511e-03]</td>\n",
       "      <td>[1.05325e-03, -5.61484e-03, 7.00493e-04]</td>\n",
       "      <td>[-3.75157e-02, -1.01765e-02, 3.28397e-03]</td>\n",
       "      <td>[-1.28863e-02, -1.24403e-02, -1.97454e-02]</td>\n",
       "      <td>[-8.72742e-04, 9.02131e-03, 1.84154e-02]</td>\n",
       "      <td>[1.01768e+00, 1.02048e+00, 1.01502e+00]</td>\n",
       "      <td>[-1.33404e-01, -1.23746e-01, -1.27280e-01]</td>\n",
       "      <td>[9.51518e-02, 8.89295e-02, 9.56540e-02]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[8.74769e-05, 8.85138e-04, 1.74963e-03]</td>\n",
       "      <td>[-3.85693e-03, -8.43764e-03, -8.32625e-03]</td>\n",
       "      <td>[-1.33334e-02, -4.32787e-03, -8.71872e-03]</td>\n",
       "      <td>[-1.94293e-02, -2.12820e-02, -1.93446e-02]</td>\n",
       "      <td>[-8.61238e-03, -4.13307e-03, -3.77546e-03]</td>\n",
       "      <td>[-1.57401e-03, -2.04138e-03, -1.40570e-03]</td>\n",
       "      <td>[1.01995e+00, 1.02087e+00, 1.02185e+00]</td>\n",
       "      <td>[-1.28731e-01, -1.33991e-01, -1.34566e-01]</td>\n",
       "      <td>[8.08414e-02, 8.93040e-02, 8.42823e-02]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            body_acc_x_train  \\\n",
       "0    [1.80852e-04, 4.04510e-03, 5.40712e-03]   \n",
       "1   [1.09375e-03, 2.41589e-03, -4.87023e-04]   \n",
       "2  [3.53127e-03, -3.14845e-03, -6.47614e-03]   \n",
       "3  [-1.77235e-03, 9.98156e-04, -4.48179e-03]   \n",
       "4    [8.74769e-05, 8.85138e-04, 1.74963e-03]   \n",
       "\n",
       "                             body_acc_y_train  \\\n",
       "0     [1.07668e-02, 6.94432e-03, 5.89227e-03]   \n",
       "1  [-4.68759e-03, -5.42598e-03, -1.51494e-03]   \n",
       "2    [4.45594e-03, 3.51417e-03, -2.68002e-03]   \n",
       "3  [-1.01854e-02, -1.03379e-03, -4.97511e-03]   \n",
       "4  [-3.85693e-03, -8.43764e-03, -8.32625e-03]   \n",
       "\n",
       "                             body_acc_z_train  \\\n",
       "0     [5.55607e-02, 4.47293e-02, 2.10005e-02]   \n",
       "1  [-2.68595e-02, -2.63214e-02, -1.89863e-02]   \n",
       "2  [-5.91458e-03, -7.74461e-03, -7.68998e-03]   \n",
       "3    [1.05325e-03, -5.61484e-03, 7.00493e-04]   \n",
       "4  [-1.33334e-02, -4.32787e-03, -8.71872e-03]   \n",
       "\n",
       "                            body_gyro_x_train  \\\n",
       "0     [3.01912e-02, 5.01847e-02, 4.64883e-02]   \n",
       "1     [1.71111e-02, 2.82541e-02, 9.92908e-03]   \n",
       "2     [2.61888e-02, 1.46342e-02, 4.93729e-02]   \n",
       "3   [-3.75157e-02, -1.01765e-02, 3.28397e-03]   \n",
       "4  [-1.94293e-02, -2.12820e-02, -1.93446e-02]   \n",
       "\n",
       "                            body_gyro_y_train  \\\n",
       "0     [6.60136e-02, 6.91739e-02, 5.57777e-02]   \n",
       "1     [6.12280e-03, 1.59305e-02, 7.87224e-03]   \n",
       "2  [-2.38341e-04, -3.04995e-03, -7.07990e-03]   \n",
       "3  [-1.28863e-02, -1.24403e-02, -1.97454e-02]   \n",
       "4  [-8.61238e-03, -4.13307e-03, -3.77546e-03]   \n",
       "\n",
       "                            body_gyro_z_train  \\\n",
       "0     [2.28586e-02, 7.72471e-03, 1.50980e-02]   \n",
       "1     [1.22682e-02, 5.68509e-03, 1.19422e-02]   \n",
       "2    [2.15890e-03, 1.26839e-03, -9.01401e-04]   \n",
       "3    [-8.72742e-04, 9.02131e-03, 1.84154e-02]   \n",
       "4  [-1.57401e-03, -2.04138e-03, -1.40570e-03]   \n",
       "\n",
       "                         total_acc_x_train  \\\n",
       "0  [1.01282e+00, 1.01697e+00, 1.01864e+00]   \n",
       "1  [1.01885e+00, 1.02052e+00, 1.01793e+00]   \n",
       "2  [1.02313e+00, 1.01645e+00, 1.01312e+00]   \n",
       "3  [1.01768e+00, 1.02048e+00, 1.01502e+00]   \n",
       "4  [1.01995e+00, 1.02087e+00, 1.02185e+00]   \n",
       "\n",
       "                            total_acc_y_train  \\\n",
       "0  [-1.23217e-01, -1.24462e-01, -1.23138e-01]   \n",
       "1  [-1.23976e-01, -1.24919e-01, -1.21316e-01]   \n",
       "2  [-1.20016e-01, -1.21304e-01, -1.27783e-01]   \n",
       "3  [-1.33404e-01, -1.23746e-01, -1.27280e-01]   \n",
       "4  [-1.28731e-01, -1.33991e-01, -1.34566e-01]   \n",
       "\n",
       "                         total_acc_z_train  y_train  \n",
       "0  [1.02934e-01, 1.07493e-01, 9.76467e-02]        5  \n",
       "1  [9.79296e-02, 9.65768e-02, 1.01621e-01]        5  \n",
       "2  [9.11167e-02, 8.82905e-02, 8.75458e-02]        5  \n",
       "3  [9.51518e-02, 8.89295e-02, 9.56540e-02]        5  \n",
       "4  [8.08414e-02, 8.93040e-02, 8.42823e-02]        5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in train_data.items()]))\n",
    "df['y_train'] = y_train\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a607d70-ba8d-4757-a5a7-b1bd43f56866",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimu_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Apply the function to the dataframe\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_input\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mapply(create_text_format, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Output a sample\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_input\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "activity_map = {\n",
    "    1: \"Walking\",\n",
    "    2: \"Walking Upstairs\",\n",
    "    3: \"Walking Downstairs\",\n",
    "    4: \"Sitting\",\n",
    "    5: \"Standing\",\n",
    "    6: \"Laying\"\n",
    "}\n",
    "def create_text_format(row):\n",
    "    imu_data = (\n",
    "        f\"Using accelerometer and gyroscope data recorded along the x, y, and z axes (linear acceleration in m/s² and angular velocity in rad/s), classify the activity being performed as one of the following: Walking, Walking Upstairs, Walking Downstairs, Sitting, Standing, or Laying.\"\n",
    "        f\"Each activity exhibits distinct motion patterns:\"\n",
    "        f\"Walking: Moderate, periodic accelerations and angular velocities in all directions.\"\n",
    "        f\"Walking Upstairs: Increased vertical motion (z-axis) compared to walking.\"\n",
    "        f\"Walking Downstairs: Irregular, high vertical impacts (z-axis).\"\n",
    "        f\"Sitting: Minimal acceleration and angular velocity, near-static.\"\n",
    "        f\"Standing: Similar to sitting, but with potential minor shifts.\"\n",
    "        f\"Laying: Negligible motion, consistent low values across all axes.\"\n",
    "        f\"The data has been segmented into 2.56-second windows, with each window characterized by extracted features like mean, variance, and frequency-domain properties. Use these patterns to identify the activity.\\n\"\n",
    "        f\"body_acc_x: {row['body_acc_x_train']}, body_acc_y: {row['body_acc_y_train']}, body_acc_z: {row['body_acc_z_train']}, \"\n",
    "        f\"body_gyro_x: {row['body_gyro_x_train']}, body_gyro_y: {row['body_gyro_y_train']}, body_gyro_z: {row['body_gyro_z_train']}, \"\n",
    "        f\"Based on these numbers, classify what the person is doing during this 2.56s?\\n\"\n",
    "    )\n",
    "    return f\"{imu_data}\"\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['text_input'] = df.apply(create_text_format, axis=1)\n",
    "\n",
    "# Output a sample\n",
    "print(df[['text_input']].head())\n",
    "\n",
    "# Now the `df['text_input']` column contains the text-based data that you can feed into the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12418314-8859-4222-9b1a-c929bdc0f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardata = df.drop(['body_acc_x_train','body_acc_y_train','body_acc_z_train','body_gyro_x_train','body_gyro_y_train','body_gyro_z_train','total_acc_x_train','total_acc_y_train','total_acc_z_train'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "debe143e-3b32-4922-94ed-37cce355d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardata = hardata.rename(columns={'y_train': 'label'})\n",
    "hardata = hardata.rename(columns={'text_input': 'text'})\n",
    "hardata['label'] = hardata['label'] - 1\n",
    "hardata = hardata[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3753b82f-78ec-42a4-9e38-6f1e02529452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "0     The data provided is measured and collected in...      4\n",
      "1     The data provided is measured and collected in...      4\n",
      "2     The data provided is measured and collected in...      4\n",
      "3     The data provided is measured and collected in...      4\n",
      "4     The data provided is measured and collected in...      4\n",
      "...                                                 ...    ...\n",
      "7347  The data provided is measured and collected in...      1\n",
      "7348  The data provided is measured and collected in...      1\n",
      "7349  The data provided is measured and collected in...      1\n",
      "7350  The data provided is measured and collected in...      1\n",
      "7351  The data provided is measured and collected in...      1\n",
      "\n",
      "[7352 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hardata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc5c841-ce2d-4760-b5d8-93346669bb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheng\\AppData\\Local\\Temp\\ipykernel_16888\\2406313597.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  hardata = hardata.groupby('label').apply(lambda x: x.sample(100, replace=True) if len(x) > 150 else x)\n"
     ]
    }
   ],
   "source": [
    "hardata = hardata.groupby('label').apply(lambda x: x.sample(100, replace=True) if len(x) > 150 else x)\n",
    "label_counts = hardata['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63ef8997-bcd4-4263-82d9-fd8214299cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         text  label\n",
      "label                                                               \n",
      "0     2485  The data provided is measured and collected in...      0\n",
      "      5794  The data provided is measured and collected in...      0\n",
      "      4485  The data provided is measured and collected in...      0\n",
      "      1244  The data provided is measured and collected in...      0\n",
      "      3540  The data provided is measured and collected in...      0\n",
      "...                                                       ...    ...\n",
      "5     742   The data provided is measured and collected in...      5\n",
      "      6885  The data provided is measured and collected in...      5\n",
      "      6337  The data provided is measured and collected in...      5\n",
      "      2284  The data provided is measured and collected in...      5\n",
      "      3688  The data provided is measured and collected in...      5\n",
      "\n",
      "[600 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hardata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0496725-2c32-486a-9bea-f09b61820dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token here\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f65306a-438d-47a5-a472-16525ff78196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 22.75 GiB is allocated by PyTorch, and 3.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 110\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m    109\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 110\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    210\u001b[0m         group,\n\u001b[0;32m    211\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         state_steps,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adamw.py:606\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    604\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 606\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    609\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 22.75 GiB is allocated by PyTorch, and 3.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class LlamaForSequenceClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.classifier = nn.Linear(base_model.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Get hidden states from the causal model\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True, return_dict=True)\n",
    "        \n",
    "        # Extract the last hidden layer (not logits)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Shape: [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "        # Use the hidden state of the last token in the sequence\n",
    "        last_hidden_state = hidden_states[:, -1, :]  # Shape: [batch_size, hidden_size]\n",
    "        \n",
    "        # Pass the last hidden state through the classification head\n",
    "        logits = self.classifier(last_hidden_state)  # Shape: [batch_size, num_labels]\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "\n",
    "\n",
    "# Tokenization and Dataset Class\n",
    "class HARDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "base_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Define the classification model\n",
    "num_labels = 6\n",
    "model = LlamaForSequenceClassification(base_model=base_model, num_labels=num_labels)\n",
    "\n",
    "# Prepare data (replace `hardata` with your actual dataset)\n",
    "texts = hardata['text'].tolist()\n",
    "labels = hardata['label'].tolist()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_data(texts, labels, tokenizer, max_length=512):\n",
    "    tokenized = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return tokenized, torch.tensor(labels)\n",
    "\n",
    "train_encodings, train_labels = tokenize_data(train_texts, train_labels, tokenizer)\n",
    "val_encodings, val_labels = tokenize_data(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HARDataset(train_encodings, train_labels)\n",
    "val_dataset = HARDataset(val_encodings, val_labels)\n",
    "\n",
    "# DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs['loss']\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42432b6d-dff3-41e1-8ad2-b310f7c46824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu\\\\tokenizer_config.json',\n",
       " './fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu\\\\special_tokens_map.json',\n",
       " './fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.save_pretrained(\"./fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu\")\n",
    "torch.save(model.classifier.state_dict(), \"./fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu/classifier.pt\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_llama3.2_1b_3sdata_casual_600_har_gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d907714-2cc6-46b0-b9e3-9116c76c52e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
