{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b53e3b-00c0-4fd0-8e86-8add0d1bf7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting sentencepiece\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/c6/97/d159c32642306ee2b70732077632895438867b3b6df282354bd550cf2a67/sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "     ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "     ---------------------------- -------- 757.8/992.0 kB 24.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- 992.0/992.0 kB 20.9 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ad6af4-61b4-4d64-aaaf-02d0ad20c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780fa92c-6b1f-4c0c-929a-6f0fb4b01509",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cace7a-1048-466c-b027-6485c41c76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\cheng\\Desktop\\202A Project\\UCI HAR Dataset\\UCI HAR Dataset\\train\\y_train.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        y_train.append(int(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39cf573-a7e3-4373-b869-88aa29d88deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r'C:\\Users\\cheng\\Desktop\\202A Project\\UCI HAR Dataset\\UCI HAR Dataset\\train\\Inertial Signals'\n",
    "\n",
    "train_data = {}\n",
    "train_names = []\n",
    "interval = 5\n",
    "num_sample = 3\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        file_lines = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = list(map(lambda x: round(float(x), 5), line.strip().split()))\n",
    "                file_lines.append(list(line[interval * x] for x in range(num_sample)))\n",
    "        train_data[filename[:-4]] = file_lines\n",
    "        train_names.append(filename[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ccdac2-e2fc-4fea-bdd6-b1783c3b284b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_acc_x_train</th>\n",
       "      <th>body_acc_y_train</th>\n",
       "      <th>body_acc_z_train</th>\n",
       "      <th>body_gyro_x_train</th>\n",
       "      <th>body_gyro_y_train</th>\n",
       "      <th>body_gyro_z_train</th>\n",
       "      <th>total_acc_x_train</th>\n",
       "      <th>total_acc_y_train</th>\n",
       "      <th>total_acc_z_train</th>\n",
       "      <th>y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.00018, 0.00405, 0.00541]</td>\n",
       "      <td>[0.01077, 0.00694, 0.00589]</td>\n",
       "      <td>[0.05556, 0.04473, 0.021]</td>\n",
       "      <td>[0.03019, 0.05018, 0.04649]</td>\n",
       "      <td>[0.06601, 0.06917, 0.05578]</td>\n",
       "      <td>[0.02286, 0.00772, 0.0151]</td>\n",
       "      <td>[1.01282, 1.01697, 1.01864]</td>\n",
       "      <td>[-0.12322, -0.12446, -0.12314]</td>\n",
       "      <td>[0.10293, 0.10749, 0.09765]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.00109, 0.00242, -0.00049]</td>\n",
       "      <td>[-0.00469, -0.00543, -0.00151]</td>\n",
       "      <td>[-0.02686, -0.02632, -0.01899]</td>\n",
       "      <td>[0.01711, 0.02825, 0.00993]</td>\n",
       "      <td>[0.00612, 0.01593, 0.00787]</td>\n",
       "      <td>[0.01227, 0.00569, 0.01194]</td>\n",
       "      <td>[1.01885, 1.02052, 1.01793]</td>\n",
       "      <td>[-0.12398, -0.12492, -0.12132]</td>\n",
       "      <td>[0.09793, 0.09658, 0.10162]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.00353, -0.00315, -0.00648]</td>\n",
       "      <td>[0.00446, 0.00351, -0.00268]</td>\n",
       "      <td>[-0.00591, -0.00774, -0.00769]</td>\n",
       "      <td>[0.02619, 0.01463, 0.04937]</td>\n",
       "      <td>[-0.00024, -0.00305, -0.00708]</td>\n",
       "      <td>[0.00216, 0.00127, -0.0009]</td>\n",
       "      <td>[1.02313, 1.01645, 1.01312]</td>\n",
       "      <td>[-0.12002, -0.1213, -0.12778]</td>\n",
       "      <td>[0.09112, 0.08829, 0.08755]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.00177, 0.001, -0.00448]</td>\n",
       "      <td>[-0.01019, -0.00103, -0.00498]</td>\n",
       "      <td>[0.00105, -0.00561, 0.0007]</td>\n",
       "      <td>[-0.03752, -0.01018, 0.00328]</td>\n",
       "      <td>[-0.01289, -0.01244, -0.01975]</td>\n",
       "      <td>[-0.00087, 0.00902, 0.01842]</td>\n",
       "      <td>[1.01768, 1.02048, 1.01502]</td>\n",
       "      <td>[-0.1334, -0.12375, -0.12728]</td>\n",
       "      <td>[0.09515, 0.08893, 0.09565]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[9e-05, 0.00089, 0.00175]</td>\n",
       "      <td>[-0.00386, -0.00844, -0.00833]</td>\n",
       "      <td>[-0.01333, -0.00433, -0.00872]</td>\n",
       "      <td>[-0.01943, -0.02128, -0.01934]</td>\n",
       "      <td>[-0.00861, -0.00413, -0.00378]</td>\n",
       "      <td>[-0.00157, -0.00204, -0.00141]</td>\n",
       "      <td>[1.01995, 1.02087, 1.02185]</td>\n",
       "      <td>[-0.12873, -0.13399, -0.13457]</td>\n",
       "      <td>[0.08084, 0.0893, 0.08428]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                body_acc_x_train                body_acc_y_train  \\\n",
       "0    [0.00018, 0.00405, 0.00541]     [0.01077, 0.00694, 0.00589]   \n",
       "1   [0.00109, 0.00242, -0.00049]  [-0.00469, -0.00543, -0.00151]   \n",
       "2  [0.00353, -0.00315, -0.00648]    [0.00446, 0.00351, -0.00268]   \n",
       "3    [-0.00177, 0.001, -0.00448]  [-0.01019, -0.00103, -0.00498]   \n",
       "4      [9e-05, 0.00089, 0.00175]  [-0.00386, -0.00844, -0.00833]   \n",
       "\n",
       "                 body_acc_z_train               body_gyro_x_train  \\\n",
       "0       [0.05556, 0.04473, 0.021]     [0.03019, 0.05018, 0.04649]   \n",
       "1  [-0.02686, -0.02632, -0.01899]     [0.01711, 0.02825, 0.00993]   \n",
       "2  [-0.00591, -0.00774, -0.00769]     [0.02619, 0.01463, 0.04937]   \n",
       "3     [0.00105, -0.00561, 0.0007]   [-0.03752, -0.01018, 0.00328]   \n",
       "4  [-0.01333, -0.00433, -0.00872]  [-0.01943, -0.02128, -0.01934]   \n",
       "\n",
       "                body_gyro_y_train               body_gyro_z_train  \\\n",
       "0     [0.06601, 0.06917, 0.05578]      [0.02286, 0.00772, 0.0151]   \n",
       "1     [0.00612, 0.01593, 0.00787]     [0.01227, 0.00569, 0.01194]   \n",
       "2  [-0.00024, -0.00305, -0.00708]     [0.00216, 0.00127, -0.0009]   \n",
       "3  [-0.01289, -0.01244, -0.01975]    [-0.00087, 0.00902, 0.01842]   \n",
       "4  [-0.00861, -0.00413, -0.00378]  [-0.00157, -0.00204, -0.00141]   \n",
       "\n",
       "             total_acc_x_train               total_acc_y_train  \\\n",
       "0  [1.01282, 1.01697, 1.01864]  [-0.12322, -0.12446, -0.12314]   \n",
       "1  [1.01885, 1.02052, 1.01793]  [-0.12398, -0.12492, -0.12132]   \n",
       "2  [1.02313, 1.01645, 1.01312]   [-0.12002, -0.1213, -0.12778]   \n",
       "3  [1.01768, 1.02048, 1.01502]   [-0.1334, -0.12375, -0.12728]   \n",
       "4  [1.01995, 1.02087, 1.02185]  [-0.12873, -0.13399, -0.13457]   \n",
       "\n",
       "             total_acc_z_train  y_train  \n",
       "0  [0.10293, 0.10749, 0.09765]        5  \n",
       "1  [0.09793, 0.09658, 0.10162]        5  \n",
       "2  [0.09112, 0.08829, 0.08755]        5  \n",
       "3  [0.09515, 0.08893, 0.09565]        5  \n",
       "4   [0.08084, 0.0893, 0.08428]        5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in train_data.items()]))\n",
    "df['y_train'] = y_train\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a607d70-ba8d-4757-a5a7-b1bd43f56866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          text_input\n",
      "0  The data provided is measured and collected in...\n",
      "1  The data provided is measured and collected in...\n",
      "2  The data provided is measured and collected in...\n",
      "3  The data provided is measured and collected in...\n",
      "4  The data provided is measured and collected in...\n"
     ]
    }
   ],
   "source": [
    "activity_map = {\n",
    "    1: \"Walking\",\n",
    "    2: \"Walking Upstairs\",\n",
    "    3: \"Walking Downstairs\",\n",
    "    4: \"Sitting\",\n",
    "    5: \"Standing\",\n",
    "    6: \"Laying\"\n",
    "}\n",
    "def create_text_format(row):\n",
    "    imu_data = (\n",
    "        f\"The data provided is measured and collected in 2.56s.\\n\"\n",
    "        f\"body_acc_x: {row['body_acc_x_train']}, body_acc_y: {row['body_acc_y_train']}, body_acc_z: {row['body_acc_z_train']}, \"\n",
    "        f\"body_gyro_x: {row['body_gyro_x_train']}, body_gyro_y: {row['body_gyro_y_train']}, body_gyro_z: {row['body_gyro_z_train']}, \"\n",
    "        f\"Based on these numbers, classify what the person is doing during this 2.56s?\\n\"\n",
    "    )\n",
    "    return f\"{imu_data}\"\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['text_input'] = df.apply(create_text_format, axis=1)\n",
    "\n",
    "# Output a sample\n",
    "print(df[['text_input']].head())\n",
    "\n",
    "# Now the `df['text_input']` column contains the text-based data that you can feed into the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12418314-8859-4222-9b1a-c929bdc0f65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardata = df.drop(['body_acc_x_train','body_acc_y_train','body_acc_z_train','body_gyro_x_train','body_gyro_y_train','body_gyro_z_train','total_acc_x_train','total_acc_y_train','total_acc_z_train'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "debe143e-3b32-4922-94ed-37cce355d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardata = hardata.rename(columns={'y_train': 'label'})\n",
    "hardata = hardata.rename(columns={'text_input': 'text'})\n",
    "hardata['label'] = hardata['label'] - 1\n",
    "hardata = hardata[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3753b82f-78ec-42a4-9e38-6f1e02529452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "0     The data provided is measured and collected in...      4\n",
      "1     The data provided is measured and collected in...      4\n",
      "2     The data provided is measured and collected in...      4\n",
      "3     The data provided is measured and collected in...      4\n",
      "4     The data provided is measured and collected in...      4\n",
      "...                                                 ...    ...\n",
      "7347  The data provided is measured and collected in...      1\n",
      "7348  The data provided is measured and collected in...      1\n",
      "7349  The data provided is measured and collected in...      1\n",
      "7350  The data provided is measured and collected in...      1\n",
      "7351  The data provided is measured and collected in...      1\n",
      "\n",
      "[7352 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hardata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc5c841-ce2d-4760-b5d8-93346669bb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheng\\AppData\\Local\\Temp\\ipykernel_6996\\2406313597.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  hardata = hardata.groupby('label').apply(lambda x: x.sample(100, replace=True) if len(x) > 150 else x)\n"
     ]
    }
   ],
   "source": [
    "hardata = hardata.groupby('label').apply(lambda x: x.sample(100, replace=True) if len(x) > 150 else x)\n",
    "label_counts = hardata['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63ef8997-bcd4-4263-82d9-fd8214299cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         text  label\n",
      "label                                                               \n",
      "0     6389  The data provided is measured and collected in...      0\n",
      "      4807  The data provided is measured and collected in...      0\n",
      "      5392  The data provided is measured and collected in...      0\n",
      "      5805  The data provided is measured and collected in...      0\n",
      "      3529  The data provided is measured and collected in...      0\n",
      "...                                                       ...    ...\n",
      "5     6347  The data provided is measured and collected in...      5\n",
      "      6367  The data provided is measured and collected in...      5\n",
      "      6128  The data provided is measured and collected in...      5\n",
      "      3133  The data provided is measured and collected in...      5\n",
      "      1211  The data provided is measured and collected in...      5\n",
      "\n",
      "[600 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(hardata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "774a8e03-1030-4909-8fbb-e2a9cbe2c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0496725-2c32-486a-9bea-f09b61820dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token here\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d7d6eab-1fde-4801-a311-6a928654c689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('meta-llama/Llama-3.2-1B', num_labels = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af2d54cd-ffdd-41cf-ae0b-c97eac5d76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bdab02e-d34e-433e-ae50-f2aa3ba8d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, labels, tokenizer, max_length=512):\n",
    "    tokenized = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return tokenized, torch.tensor(labels)\n",
    "\n",
    "class HARDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "texts = hardata['text'].tolist()\n",
    "labels = hardata['label'].tolist()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_encodings, train_labels = tokenize_data(train_texts, train_labels, tokenizer)\n",
    "val_encodings, val_labels = tokenize_data(val_texts, val_labels, tokenizer)\n",
    "#train_dataset = HARDataset(train_encodings, train_labels)\n",
    "#val_dataset = HARDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9adc143-6173-44cc-a8e7-0ad3539214ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "# Move encodings and labels to GPU\n",
    "train_encodings = {key: val.to(device) for key, val in train_encodings.items()}\n",
    "train_labels = train_labels.to(device)\n",
    "val_encodings = {key: val.to(device) for key, val in val_encodings.items()}\n",
    "val_labels = val_labels.to(device)\n",
    "\n",
    "train_dataset = HARDataset(train_encodings, train_labels)\n",
    "val_dataset = HARDataset(val_encodings, val_labels)\n",
    "\n",
    "# DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b7437af-d5bc-4161-9ef7-f83437217887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 4.822123463948568\n",
      "Epoch 2/3, Loss: 2.1761063893636066\n",
      "Epoch 3/3, Loss: 1.975742554664612\n",
      "Validation Accuracy: 0.1417\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Move batch data to GPU\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Validation Accuracy: {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d67d45f-8d53-4d5a-8ec6-6fe4bcb5603d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 3.3960912108421324\n",
      "Epoch 2/2, Loss: 1.395221757888794\n",
      "Validation Accuracy: 0.3250\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # Move batch data to GPU\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Validation Accuracy: {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1961d6a-3c34-4698-985a-64110b7fc53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu\\\\tokenizer_config.json',\n",
       " './fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu\\\\special_tokens_map.json',\n",
       " './fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27d47317-e82c-4e47-a13c-98b7777826c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu.zip' target='_blank'>fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu.zip</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\cheng\\Desktop\\llama 3.2 1B casual model\\fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu.zip"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shutil import make_archive\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Zip the saved model\n",
    "make_archive(\"fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu\", 'zip', \"./fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu\")\n",
    "\n",
    "# Create a download link for the zipped file\n",
    "FileLink(\"fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ba129f-0ad1-4537-9c9e-fdec57427617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal LM model saved to C:\\Users\\cheng\\Desktop\\llama 3.2 1B casual model\\fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu_gg\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Path to your fine-tuned model\n",
    "sequence_model_path = r\"C:\\Users\\cheng\\Desktop\\llama 3.2 1B casual model\\fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu\"\n",
    "output_path = r\"C:\\Users\\cheng\\Desktop\\llama 3.2 1B casual model\\fine_tuned_llama3.2_1b_auto_3sdata_600sample_har_gpu_gg\"\n",
    "\n",
    "# Load the sequence classification model and tokenizer\n",
    "sequence_model = AutoModelForSequenceClassification.from_pretrained(sequence_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(sequence_model_path)\n",
    "\n",
    "# Save the model as a causal language model\n",
    "sequence_model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n",
    "\n",
    "print(f\"Causal LM model saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc1d26-eb04-4134-a9a2-50736ae97ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6976181-063a-4bbd-ac67-5b2eb5f8f5a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gguf_converter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Hypothetical GGUF library\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgguf_converter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GGUFConverter\n\u001b[0;32m      4\u001b[0m gguf_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/fine_tuned_model_gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m converter \u001b[38;5;241m=\u001b[39m GGUFConverter(model)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gguf_converter'"
     ]
    }
   ],
   "source": [
    "# Hypothetical GGUF library\n",
    "from gguf_converter import GGUFConverter\n",
    "\n",
    "gguf_save_path = \"/content/fine_tuned_model_gguf\"\n",
    "converter = GGUFConverter(model)\n",
    "converter.save(gguf_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42432b6d-dff3-41e1-8ad2-b310f7c46824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
